# Testing Kit - Execution Plan

**Purpose:** 12-phase systematic validation for GO/NO-GO deployment decision
**Timeline:** 3-4 hours (comprehensive path)
**Last Updated:** 2025-10-20
**Status:** Active

---

## Table of Contents

1. [Overview](#overview)
2. [Testing Strategy](#testing-strategy)
3. [Phase Breakdown](#phase-breakdown)
4. [Decision Gates](#decision-gates)
5. [GO/NO-GO Criteria](#gono-go-criteria)
6. [Execution Notes](#execution-notes)

---

## Overview

This execution plan provides a **systematic, bottom-up testing strategy** to validate the Apex Memory System before deployment. Each phase has clear success criteria, decision gates, and failure handling procedures.

### Objectives

- ‚úÖ Validate all 6 architectural layers independently
- ‚úÖ Test critical integration points
- ‚úÖ Verify performance targets
- ‚úÖ Make data-driven GO/NO-GO decision
- ‚úÖ Document all failures with production impact

### Scope

**6 Architectural Layers:**
1. Database Writers (PostgreSQL, Neo4j, Qdrant, Redis)
2. Services (Enhanced Saga, Graphiti, Embeddings, Staging)
3. Temporal Activities (9 activities)
4. Workflows (DocumentIngestion + StructuredDataIngestion)
5. API Endpoints (REST interface)
6. Query Router (Intent classification, caching)

**Additional Testing:**
- Integration points (cross-layer validation)
- Load & chaos testing (resilience)
- Metrics & observability (production readiness)

### Timeline Options

| Path | Duration | Phases Included |
|------|----------|-----------------|
| **Quick** | 1 hour | Pre-flight + Enhanced Saga + E2E workflows only |
| **Comprehensive** | 3-4 hours | All layers + integration (RECOMMENDED) |
| **Full** | 5-6 hours | Comprehensive + load/chaos + detailed troubleshooting |

**Recommended:** Comprehensive (3-4 hours)

---

## Testing Strategy

### Bottom-Up Approach

**Principle:** Test foundation first, build up to higher layers.

```
Layer 6: Query Router
Layer 5: API Endpoints
Layer 4: Workflows (orchestration)
Layer 3: Activities (unit of work)
Layer 2: Services (business logic)
Layer 1: Database Writers (foundation)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Pre-Flight: Infrastructure
```

**Why?** If Layer 1 fails, everything else will fail. Fix foundation before testing upper layers.

### Fix-and-Document Workflow

For every test failure:

1. **Document Problem** - Capture error, context, environment
2. **Root Cause Analysis** - Identify underlying issue (not symptoms)
3. **Apply Fix** - Implement solution (production code or test code)
4. **Validate Fix** - Confirm test now passes
5. **Document Outcome** - Production impact, future considerations

**Results Documented In:** `results/RESULTS-TEMPLATE.md`

### Decision Gates

**Between each phase, evaluate:**
- Did all tests pass?
- Are there blockers for the next phase?
- Should testing continue or stop to fix issues?

**Critical Decision Gates:**
- **Gate 1:** Infrastructure ready?
- **Gate 2:** Database writers functional?
- **Gate 3:** Enhanced Saga 121/121? (MANDATORY)
- **Gate 4:** Activities functional?
- **Gate 5:** Workflows E2E successful?
- **Gate 6-8:** API, Query Router, Integration functional?

---

## Phase Breakdown

### Phase 0: Pre-Testing Setup (15 min)

**Objective:** Prepare monitoring and results recording infrastructure.

**Activities:**
- [ ] Open `results/RESULTS-TEMPLATE.md` for recording
- [ ] Open Grafana dashboard (http://localhost:3001/d/temporal-ingestion)
- [ ] Open Temporal UI (http://localhost:8088)
- [ ] Open Prometheus (http://localhost:9090)
- [ ] Prepare terminal windows for parallel monitoring

**Success Criteria:**
- ‚úÖ All monitoring interfaces accessible
- ‚úÖ Results template ready
- ‚úÖ Environment prepared for testing

**What You'll Know:**
- Observability infrastructure is ready
- Can monitor tests in real-time

---

### Phase 1: Pre-Flight Validation (15 min)

**Objective:** Ensure all infrastructure is healthy before testing begins.

**Tests:**
- [ ] PostgreSQL: Running + Connectable (psql check)
- [ ] Neo4j: Running + Connectable (browser login)
- [ ] Qdrant: Running + Connectable (API check)
- [ ] Redis: Running + Connectable (redis-cli ping)
- [ ] Temporal Server: Healthy (temporal server health)
- [ ] Temporal Worker: Running (ps aux check)
- [ ] API Server: Responding (/health endpoint)
- [ ] Staging Directory: Exists + Writable (/tmp/apex-staging)

**Success Criteria:**
- ‚úÖ ALL 8 checks pass
- ‚úÖ No infrastructure failures

**What You'll Know:**
- Infrastructure is healthy and ready for testing
- All required services are running

**If Failed:**
- ‚ùå STOP immediately
- Fix infrastructure before proceeding
- Document failure in results template

**Decision Gate 1:** Infrastructure ready?
- **YES** ‚Üí Proceed to Phase 2
- **NO** ‚Üí Fix infrastructure, re-run Phase 1

**Reference:** `IMPLEMENTATION.md` - "Pre-Flight Validation"

---

### Phase 2: Layer 1 - Database Writers (30 min)

**Objective:** Validate each database writer works independently.

**Tests:**

**PostgresWriter:**
- [ ] Connection pool initialization
- [ ] Write document + embedding
- [ ] Write chunks

**Neo4jWriter:**
- [ ] Connection initialization
- [ ] Write document node
- [ ] Write entities + relationships

**QdrantWriter:**
- [ ] Connection initialization
- [ ] Write embedding (1536 dims)

**RedisWriter:**
- [ ] Connection initialization
- [ ] Cache operations (set/get/delete)

**Success Criteria:**
- ‚úÖ All 4 writers functional independently
- ‚úÖ No connection errors
- ‚úÖ All write operations succeed

**What You'll Know:**
- Each database is independently writable
- Database clients configured correctly
- Foundation is solid for higher layers

**If Failed:**
- Identify which writer failed
- Fix database configuration/connectivity
- Re-test failed writer in isolation
- Document in results template

**Decision Gate 2:** All database writers working?
- **YES** ‚Üí Proceed to Phase 3
- **NO** ‚Üí Fix databases, re-run Phase 2

**Reference:** `TESTING-KIT.md` - "Layer 1: Database Writers"

---

### Phase 3: Layer 2 - Services & Enhanced Saga (30 min) üö® CRITICAL

**Objective:** Validate business logic services, especially Enhanced Saga pattern.

**Tests:**

**Enhanced Saga Baseline (MANDATORY):**
- [ ] Run: `pytest tests/ --ignore=tests/load/ --ignore=tests/integration/ -v`
- [ ] Expected: **121/121 tests passing**
- [ ] Test coverage: Distributed locking, idempotency, circuit breakers, rollback, Graphiti rollback

**Graphiti Service:**
- [ ] Document extraction (expect 2-4 entities)
- [ ] JSON extraction

**Embedding Service:**
- [ ] Single embedding (1536 dims)
- [ ] Batch embeddings

**Staging Manager:**
- [ ] Create staging directory
- [ ] Metadata tracking
- [ ] TTL cleanup

**Success Criteria:**
- ‚úÖ **121/121 Enhanced Saga tests passing (MANDATORY)**
- ‚úÖ Graphiti extracting entities (90%+ accuracy)
- ‚úÖ Embeddings generating correctly
- ‚úÖ Staging lifecycle functional

**What You'll Know:**
- Enhanced Saga pattern protecting data integrity
- LLM extraction achieving high accuracy
- Services ready for workflow integration

**If Enhanced Saga Fails:**
- ‚ùå **AUTOMATIC NO-GO FOR DEPLOYMENT**
- Identify failing tests: `pytest tests/ --ignore=tests/load/ --ignore=tests/integration/ -v | grep FAILED`
- Run failed tests with `--tb=long` for full traceback
- Fix production code issues
- Re-run all 121 tests
- Document all fixes in results template
- **Do NOT proceed until 121/121 passing**

**Decision Gate 3:** Enhanced Saga 121/121?
- **YES** ‚Üí Proceed to Phase 4
- **NO** ‚Üí **STOP - FIX BEFORE CONTINUING** (automatic NO-GO)

**Reference:** `TESTING-KIT.md` - "Layer 2: Services"

---

### Phase 4: Layer 3 - Temporal Activities (30 min)

**Objective:** Test each of the 9 activities in isolation.

**Tests:**

**Document Activities:**
- [ ] pull_and_stage_document_activity
- [ ] parse_document_activity
- [ ] extract_entities_activity (Graphiti)
- [ ] generate_embeddings_activity
- [ ] write_to_databases_activity (Enhanced Saga)
- [ ] cleanup_staging_activity

**JSON Activities:**
- [ ] fetch_structured_data_activity
- [ ] extract_entities_from_json_activity
- [ ] write_structured_data_activity

**Success Criteria:**
- ‚úÖ All 9 activities execute without errors
- ‚úÖ Correct output structures returned
- ‚úÖ Metrics being recorded

**What You'll Know:**
- Activities work independently
- Ready for workflow orchestration
- Activity ‚Üí service delegation functional

**If Failed:**
- Test failed activity in isolation
- Check service layer dependencies
- Verify activity ‚Üí service delegation
- Document failure and fix

**Decision Gate 4:** All activities functional?
- **YES** ‚Üí Proceed to Phase 5
- **NO** ‚Üí Fix activities, re-run Phase 4

**Reference:** `TESTING-KIT.md` - "Layer 3: Temporal Activities"

---

### Phase 5: Layer 4 - Workflows E2E (30 min)

**Objective:** Test complete workflows end-to-end via Temporal.

**Tests:**

**DocumentIngestionWorkflow:**
- [ ] Create test document
- [ ] Run 6-step workflow (Stage ‚Üí Parse ‚Üí Extract ‚Üí Embed ‚Üí Write ‚Üí Cleanup)
- [ ] Verify all activities executed
- [ ] Check Temporal UI for completion
- [ ] Verify staging cleaned up

**StructuredDataIngestionWorkflow:**
- [ ] Create test JSON payload
- [ ] Run 4-step workflow (Fetch ‚Üí Extract ‚Üí Embed ‚Üí Write)
- [ ] Verify entities extracted
- [ ] Check databases written

**Integration Test Suite:**
- [ ] Run `pytest tests/integration/ -v -m integration`
- [ ] Validate E2E scenarios
- [ ] Check staging lifecycle

**Success Criteria:**
- ‚úÖ Both workflows complete E2E successfully
- ‚úÖ All activities executed in sequence
- ‚úÖ Staging cleaned up
- ‚úÖ All 4 databases written
- ‚úÖ Workflows visible in Temporal UI

**What You'll Know:**
- Complete ingestion pipeline functional
- Orchestration working correctly
- Both document and JSON workflows operational

**If Failed:**
- Check Temporal UI for activity failures
- Review worker logs
- Verify activity execution order
- Check database write results
- Document failure and fix

**Decision Gate 5:** Both workflows E2E successful?
- **YES** ‚Üí Proceed to Phase 6
- **NO** ‚Üí Fix orchestration, re-run Phase 5

**Reference:** `IMPLEMENTATION.md` - "Layer 4: Workflows"

---

### Phase 6: Layer 5 - API Endpoints (15 min)

**Objective:** Validate FastAPI endpoints work correctly.

**Tests:**
- [ ] GET /api/v1/health ‚Üí Database connectivity
- [ ] POST /api/v1/ingest ‚Üí Document upload triggers workflow
- [ ] POST /api/v1/ingest-structured ‚Üí JSON upload triggers workflow
- [ ] GET /api/v1/workflow/{id}/status ‚Üí Status tracking
- [ ] Error handling (invalid file types, missing fields)

**Success Criteria:**
- ‚úÖ All endpoints responding correctly
- ‚úÖ Workflows triggered from API
- ‚úÖ Correct HTTP status codes
- ‚úÖ Workflow IDs returned

**What You'll Know:**
- User-facing API functional
- API ‚Üí Temporal integration working
- Error handling correct

**If Failed:**
- Check FastAPI logs
- Verify Temporal client connection
- Test endpoint in isolation
- Document failure

**Decision Gate 6:** API layer functional?
- **YES** ‚Üí Proceed to Phase 7
- **NO** ‚Üí Fix API, re-run Phase 6

**Reference:** `IMPLEMENTATION.md` - "Layer 5: API Endpoints"

---

### Phase 7: Layer 6 - Query Router (15 min)

**Objective:** Validate intent classification and database routing.

**Tests:**
- [ ] LLM intent classifier (4 intent types: graph, temporal, semantic, metadata)
- [ ] Database routing logic (correct DB for each intent)
- [ ] Cache performance (write/read/TTL)

**Success Criteria:**
- ‚úÖ Intent classification >80% confidence
- ‚úÖ Correct database routing for each intent
- ‚úÖ Cache hit/miss working

**What You'll Know:**
- Query router correctly classifying queries
- Cache reducing latency
- Intent-based routing functional

**If Failed:**
- Check Claude API key
- Verify routing logic
- Test cache operations
- Document failure

**Decision Gate 7:** Query router functional?
- **YES** ‚Üí Proceed to Phase 8
- **NO** ‚Üí Fix routing, re-run Phase 7

**Reference:** `IMPLEMENTATION.md` - "Layer 6: Query Router"

---

### Phase 8: Integration Testing - Cross-Layer Validation (30 min)

**Objective:** Test critical integration points between layers.

**Tests:**

**Critical Integration Points:**
- [ ] API ‚Üí Temporal: Workflow triggering (upload triggers workflow in Temporal UI)
- [ ] Temporal ‚Üí Services: Activity delegation (activities call services)
- [ ] Enhanced Saga ‚Üí 4 Databases: Parallel writes (all DBs written simultaneously)
- [ ] Graphiti ‚Üí Neo4j: Entity storage (extracted entities in graph)
- [ ] Redis ‚Üí Components: Locks + cache (distributed locking + caching working)

**Success Criteria:**
- ‚úÖ All integration points functional
- ‚úÖ Data flows end-to-end correctly
- ‚úÖ No integration failures

**What You'll Know:**
- System integrated correctly
- All layers communicating
- No broken integration points

**If Failed:**
- Identify which integration point failed
- Check connection between layers
- Verify data flow
- Document failure

**Decision Gate 8:** All integrations working?
- **YES** ‚Üí Proceed to Phase 9 (optional) or Phase 10
- **NO** ‚Üí Fix integration, re-run Phase 8

**Reference:** `IMPLEMENTATION.md` - "Integration Testing"

---

### Phase 9: Load & Chaos Testing - Resilience Validation (1 hour, optional)

**Objective:** Validate system resilience under stress and failures.

**Tests:**

**Load Testing:**
- [ ] Concurrent workflow execution (10+ docs/sec target)
- [ ] Run: `pytest tests/load/test_concurrent_workflows.py -v -m load`
- [ ] Verify no resource exhaustion
- [ ] Check metrics accuracy under load

**Chaos Testing:**
- [ ] Database failure scenarios (stop PostgreSQL, trigger workflow)
- [ ] Saga rollback validation (verify rollback on DB failure)
- [ ] System recovery (restart DB, retry workflow)
- [ ] Network partition simulation (optional)
- [ ] Resource exhaustion (optional)

**Success Criteria:**
- ‚úÖ Target throughput achieved (10+ docs/sec)
- ‚úÖ Graceful degradation on failures
- ‚úÖ System recovers after DB restart
- ‚úÖ Saga rollback prevents partial writes

**What You'll Know:**
- System resilient under stress
- Failure handling working correctly
- Production-ready for concurrent load

**Note:** This phase is **OPTIONAL** - can deploy without if critical tests pass.

**If Failed:**
- Review performance bottlenecks
- Optimize slow components
- Document performance issues

**Decision Gate 9:** Load/chaos tests acceptable?
- **YES** ‚Üí Proceed to Phase 10
- **NO (but critical tests passed)** ‚Üí Proceed to Phase 10 (deploy with monitoring)
- **NO (critical tests failed)** ‚Üí Fix critical issues first

**Reference:** `TESTING-KIT.md` - "Load & Chaos Testing"

---

### Phase 10: Metrics & Observability Validation (15 min)

**Objective:** Validate monitoring and alerting infrastructure.

**Tests:**
- [ ] Grafana dashboard displaying correctly (http://localhost:3001/d/temporal-ingestion)
- [ ] Prometheus metrics recording (check 27 Temporal metrics)
- [ ] Alert rules configured (12 alert rules present)
- [ ] Test alert triggering (cause failure, verify alert fires)
- [ ] No silent failures (zero chunks/entities alerts configured)

**Success Criteria:**
- ‚úÖ All metrics recording
- ‚úÖ Dashboards functional (33 panels visible)
- ‚úÖ Alerts firing when triggered

**What You'll Know:**
- Complete observability in place
- Production monitoring ready
- Alerts will notify of failures

**If Failed:**
- Check Prometheus scraping
- Verify Grafana data source
- Review alert rule syntax
- Document missing metrics

**Decision Gate 10:** Metrics/observability ready?
- **YES** ‚Üí Proceed to Phase 11
- **NO** ‚Üí Fix monitoring, re-run Phase 10

**Reference:** `TESTING-KIT.md` - "Debugging Procedures"

---

### Phase 11: Results Analysis & GO/NO-GO Decision (30 min)

**Objective:** Analyze all test results and make final deployment decision.

**Activities:**
1. [ ] Review `results/RESULTS-TEMPLATE.md` (all layers)
2. [ ] Calculate pass rate (target: 100% critical tests)
3. [ ] Check performance metrics vs. targets
4. [ ] Review documented failures (if any)
5. [ ] Assess production impact of failures
6. [ ] Make final GO/NO-GO decision

**Analysis Checklist:**
- [ ] Enhanced Saga: 121/121 passing?
- [ ] All databases: Healthy?
- [ ] Workflows: Both E2E successful?
- [ ] API: All endpoints functional?
- [ ] Metrics: Recording correctly?
- [ ] Zero critical errors in logs?

**Decision Outcome:**

**‚úÖ GO FOR DEPLOYMENT** (ALL must be true):
- Enhanced Saga: 121/121 ‚úÖ
- All databases: Healthy ‚úÖ
- Workflows: E2E success ‚úÖ
- API: Functional ‚úÖ
- Metrics: Recording ‚úÖ
- Zero critical errors ‚úÖ

**Confidence:** HIGH
**Risk:** LOW
**Recommendation:** PROCEED

---

**‚ùå NO-GO - DO NOT DEPLOY** (ANY triggers NO-GO):
- Enhanced Saga: X/121 ‚ùå
- Database unreachable ‚ùå
- Workflows failing ‚ùå
- Metrics not recording ‚ùå

**Confidence:** LOW
**Risk:** HIGH
**Recommendation:** FIX ISSUES FIRST

---

**‚ö†Ô∏è CONDITIONAL GO** (critical pass, optional fail):
- Critical tests: All passing ‚úÖ
- Load testing: 8 docs/sec (target 10) ‚ö†Ô∏è
- Cache hit rate: 45% (target 60%) ‚ö†Ô∏è

**Confidence:** MEDIUM
**Risk:** MEDIUM
**Recommendation:** DEPLOY with monitoring, optimize later

**Reference:** `IMPLEMENTATION.md` - "Deployment Decision"

---

### Phase 12: Documentation & Handoff (15 min)

**Objective:** Finalize documentation and prepare for deployment.

**If GO:**
- [ ] Finalize `results/RESULTS-TEMPLATE.md`
- [ ] Complete deployment readiness checklist
- [ ] Record all test outcomes with timestamps
- [ ] Note performance optimization opportunities
- [ ] Document known issues (if any)
- [ ] Create handoff documentation

**If NO-GO:**
- [ ] Document all failures with production impact
- [ ] Create fix plan with priorities (critical ‚Üí optional)
- [ ] Estimate re-test timeline
- [ ] Schedule re-testing after fixes
- [ ] Communicate blockers to stakeholders

**Deliverables:**
- ‚úÖ Completed results template
- ‚úÖ GO/NO-GO decision with confidence level
- ‚úÖ Risk assessment
- ‚úÖ Deployment readiness checklist (if GO)
- ‚úÖ Fix plan with priorities (if NO-GO)

**Reference:** `results/RESULTS-TEMPLATE.md`

---

## Decision Gates

### Gate Summary

| Gate | Phase | Critical Question | GO Criteria | NO-GO Action |
|------|-------|------------------|-------------|--------------|
| **1** | Pre-Flight | Infrastructure ready? | All services running | Fix infrastructure |
| **2** | Layer 1 | Database writers working? | All 4 writers functional | Fix database config |
| **3** | Layer 2 | Enhanced Saga 121/121? | ALL tests passing | **STOP - FIX SAGA** |
| **4** | Layer 3 | Activities functional? | All 9 activities passing | Fix activities |
| **5** | Layer 4 | Workflows E2E success? | Both workflows complete | Fix orchestration |
| **6** | Layer 5 | API functional? | All endpoints working | Fix API |
| **7** | Layer 6 | Query router working? | Intent classification OK | Fix routing |
| **8** | Integration | All integrations OK? | No broken connections | Fix integration |
| **9** | Load/Chaos | Performance acceptable? | Targets met (optional) | Optimize or deploy |
| **10** | Observability | Metrics recording? | All metrics present | Fix monitoring |
| **11** | Analysis | Deploy? | See GO criteria | See NO-GO criteria |

### Critical Decision Gates

**Gate 3 (Enhanced Saga) is MANDATORY:**
- If 121/121 not passing ‚Üí **Automatic NO-GO**
- Data integrity risk is unacceptable
- Must fix before deployment

**Other Gates are Progressive:**
- Fix issue, re-run phase, continue if fixed
- Document all fixes in results template

---

## GO/NO-GO Criteria

### ‚úÖ GO FOR DEPLOYMENT

**ALL of these must be true:**

1. **Enhanced Saga: 121/121 tests passing** (MANDATORY)
2. **All databases: Healthy and connectable**
3. **Workflows: Both DocumentIngestion + StructuredDataIngestion E2E success**
4. **API: All endpoints responding correctly**
5. **Metrics: Recording correctly in Prometheus**
6. **Zero critical errors in logs during test runs**

**Confidence Level:** HIGH
**Risk Level:** LOW
**Deployment Recommendation:** PROCEED WITH CONFIDENCE

---

### ‚ùå NO-GO - DO NOT DEPLOY

**If ANY of these are true:**

1. **Enhanced Saga tests failing** (data integrity risk)
2. **Any database unreachable** (system won't function)
3. **Workflows not completing E2E** (ingestion broken)
4. **Graphiti extraction failing** (accuracy <90%)
5. **Metrics not recording** (no observability)
6. **Orphaned resources accumulating** (resource leak)

**Confidence Level:** LOW
**Risk Level:** HIGH
**Deployment Recommendation:** FIX ISSUES FIRST, RE-TEST

---

### ‚ö†Ô∏è CONDITIONAL GO

**If critical criteria pass, but optional criteria fail:**

**Critical:** ‚úÖ All passing
**Optional:** ‚ùå Partial failures (e.g., load tests, cache hit rate)

**Examples:**
- Load testing: 8 docs/sec (target 10+)
- Cache hit rate: 45% (target 60%)
- Query latency: 1.5s P90 (target <1s)

**Confidence Level:** MEDIUM
**Risk Level:** MEDIUM
**Deployment Recommendation:** DEPLOY with close monitoring, optimize post-deployment

**Mitigation:**
- Monitor performance metrics closely
- Set up alerts for performance degradation
- Plan optimization sprint post-deployment
- Document known performance limitations

---

## Execution Notes

### Testing Path Selection

**Quick Path (1 hour):**
- Phases 1, 3, 5 only
- Critical tests only: Pre-flight + Enhanced Saga + E2E workflows
- Use when: Confident in system, need quick validation

**Comprehensive Path (3-4 hours) - RECOMMENDED:**
- Phases 1-8, 10-12
- All layers + integration
- Skip Phase 9 (load/chaos) if time-constrained
- Use when: First deployment, major changes, production readiness

**Full Path (5-6 hours):**
- All phases 1-12
- Includes load/chaos testing
- Detailed troubleshooting
- Use when: Critical deployment, need maximum confidence

### Documentation Protocol

**For Every Test:**
- Record result in `results/RESULTS-TEMPLATE.md`
- Note timestamp
- Record metrics (if applicable)

**For Every Failure:**
1. Document error message
2. Identify root cause
3. Apply fix
4. Document production impact
5. Re-test to confirm fix

**Fix-and-Document Template:**
```markdown
## Failure: [Test Name]

**Error:** [Error message]
**Root Cause:** [Why it failed]
**Fix Applied:** [What was changed]
**Production Impact:** [Was production code affected?]
**Validation:** [How was fix confirmed?]
```

### Monitoring During Tests

**Keep Open:**
- Grafana: http://localhost:3001/d/temporal-ingestion
- Temporal UI: http://localhost:8088
- Prometheus: http://localhost:9090
- Terminal: Worker logs, API logs

**Watch For:**
- Metric spikes (latency, errors)
- Workflow failures in Temporal UI
- Database connection errors
- Resource exhaustion warnings

### Baseline Test Preservation

**Critical Principle:** 121 Enhanced Saga tests must ALWAYS pass.

**During Testing:**
- If ANY production code changes made ‚Üí Re-run 121 tests
- If 121 tests fail after fix ‚Üí Fix introduced regression
- Preserve baseline: 121 tests are the foundation

### Context Compact Recommendation

**After Phase 12 (if GO):**
- Create context compact before deployment
- Preserve:
  - Final results template
  - GO/NO-GO decision
  - Performance metrics
  - Known issues
- Archive testing session for future reference

---

## Final Output

### Expected Deliverables

1. **Completed `results/RESULTS-TEMPLATE.md`**
   - All phases recorded
   - All metrics documented
   - All failures documented with fixes

2. **GO/NO-GO Decision**
   - Clear recommendation (GO/NO-GO/CONDITIONAL)
   - Confidence level (HIGH/MEDIUM/LOW)
   - Risk assessment
   - Supporting evidence

3. **Deployment Readiness Checklist** (if GO)
   - All prerequisites met
   - Known issues documented
   - Monitoring configured
   - Rollback plan prepared

4. **Fix Plan with Priorities** (if NO-GO)
   - Critical fixes (must fix before deployment)
   - High priority (should fix before deployment)
   - Medium priority (can fix post-deployment)
   - Low priority (nice to have)

5. **Performance Baseline**
   - Actual metrics vs. targets
   - Optimization opportunities
   - Monitoring plan

---

## Quick Reference

### Critical Commands

**Start All Services:**
```bash
cd apex-memory-system/docker
docker-compose up -d
sleep 30
```

**Enhanced Saga Baseline:**
```bash
cd apex-memory-system
pytest tests/ --ignore=tests/load/ --ignore=tests/integration/ -v
# Expected: 121/121 passing
```

**Integration Tests:**
```bash
pytest tests/integration/ -v -m integration
```

**Check Metrics:**
```bash
# Prometheus
curl http://localhost:9090/api/v1/query?query=temporal_workflow_completed_total

# Grafana
open http://localhost:3001/d/temporal-ingestion
```

### Emergency Procedures

**If Enhanced Saga Fails:**
```bash
# Identify failing tests
pytest tests/ --ignore=tests/load/ --ignore=tests/integration/ -v | grep FAILED

# Run specific test with full output
pytest tests/unit/test_database_writer.py::test_name -v --tb=long

# Fix code, re-run ALL 121 tests
pytest tests/ --ignore=tests/load/ --ignore=tests/integration/ -v
```

**If Database Down:**
```bash
# Restart all databases
cd apex-memory-system/docker
docker-compose restart

# Wait for initialization
sleep 30

# Re-run connectivity checks
```

**If Worker Not Responding:**
```bash
# Kill worker
pkill -f dev_worker

# Restart worker
cd apex-memory-system
PYTHONPATH=src:$PYTHONPATH python src/apex_memory/temporal/workers/dev_worker.py &
```

---

## Next Steps

**To Begin Testing:**

1. **Review this execution plan** - Understand all 12 phases
2. **Open monitoring interfaces** - Grafana, Temporal UI, Prometheus
3. **Prepare results template** - `results/RESULTS-TEMPLATE.md`
4. **Start Phase 1** - Pre-Flight Validation
5. **Follow phases sequentially** - Don't skip decision gates
6. **Document everything** - Use fix-and-document workflow
7. **Make GO/NO-GO decision** - Based on objective criteria

**For Detailed Commands:**
- See `IMPLEMENTATION.md` - Step-by-step execution guide
- See `TESTING-KIT.md` - Comprehensive reference

---

**Good luck with testing! üöÄ**

**Last Updated:** 2025-10-20
**Version:** 1.0
**Status:** Active
