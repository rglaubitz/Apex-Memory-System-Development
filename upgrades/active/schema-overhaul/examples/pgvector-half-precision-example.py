"""
pgvector Half-Precision Vectors Example

Source: NEW-FEATURES-2025.md (Section 1.1 Half-Precision Vectors)
Source: postgresql-research.md (pgvector 0.8.1 features)
Verified: November 2025

This example demonstrates pgvector 0.8.1's half-precision vector support,
which provides 50% memory savings with <1% accuracy loss.

Features:
- HALFVEC(1536) type for 16-bit floats
- 50% memory reduction (6 KB ‚Üí 3 KB per vector)
- HNSW index support for half-precision
- Migration from full precision to half precision
- Accuracy benchmarking

Requirements:
    pip install psycopg2-binary pgvector numpy scikit-learn
    PostgreSQL 14+ with pgvector 0.8.1
"""

import psycopg2
from psycopg2.extras import execute_values
import numpy as np
from typing import List, Tuple
import time


class PgVectorHalfPrecision:
    """
    Demonstrates pgvector half-precision vector operations.

    Half-precision (HALFVEC) uses 16-bit floats instead of 32-bit floats:
    - Memory: 1536 √ó 2 bytes = 3,072 bytes per vector (vs. 6,144 bytes full precision)
    - Accuracy: <1% loss for most embeddings
    - Speed: Faster index builds due to smaller data size
    """

    def __init__(self, conn_string: str):
        """
        Initialize connection to PostgreSQL.

        Args:
            conn_string: PostgreSQL connection string
        """
        self.conn = psycopg2.connect(conn_string)
        self.conn.autocommit = False

    def create_half_precision_table(self):
        """
        Create table with half-precision embeddings.

        Uses HALFVEC(1536) type for OpenAI text-embedding-3-small.
        """
        with self.conn.cursor() as cur:
            # Enable pgvector extension
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector")

            # Create table with half-precision embeddings
            cur.execute("""
                CREATE TABLE IF NOT EXISTS documents_half (
                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    title TEXT NOT NULL,
                    content TEXT,
                    embedding HALFVEC(1536) NOT NULL,  -- Half-precision (16-bit floats)
                    created_at TIMESTAMP DEFAULT NOW()
                )
            """)

            # Create HNSW index for fast similarity search
            print("Creating HNSW index (may take a few minutes for large datasets)...")
            cur.execute("""
                CREATE INDEX IF NOT EXISTS documents_half_embedding_hnsw_idx
                ON documents_half USING hnsw (embedding halfvec_cosine_ops)
                WITH (m = 16, ef_construction = 64)
            """)

            self.conn.commit()
            print("‚úÖ Half-precision table and index created")

    def create_full_precision_table(self):
        """
        Create table with full-precision embeddings for comparison.

        Uses VECTOR(1536) type (32-bit floats).
        """
        with self.conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS documents_full (
                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    title TEXT NOT NULL,
                    content TEXT,
                    embedding VECTOR(1536) NOT NULL,  -- Full precision (32-bit floats)
                    created_at TIMESTAMP DEFAULT NOW()
                )
            """)

            print("Creating HNSW index for full precision...")
            cur.execute("""
                CREATE INDEX IF NOT EXISTS documents_full_embedding_hnsw_idx
                ON documents_full USING hnsw (embedding vector_cosine_ops)
                WITH (m = 16, ef_construction = 64)
            """)

            self.conn.commit()
            print("‚úÖ Full-precision table and index created")

    def insert_documents_half(self, documents: List[Tuple[str, str, np.ndarray]]):
        """
        Insert documents with half-precision embeddings.

        Args:
            documents: List of (title, content, embedding) tuples
        """
        with self.conn.cursor() as cur:
            execute_values(
                cur,
                """
                INSERT INTO documents_half (title, content, embedding)
                VALUES %s
                """,
                [(title, content, embedding.tolist()) for title, content, embedding in documents],
                template="(%s, %s, %s::halfvec)"  # Cast to halfvec
            )

        self.conn.commit()
        print(f"‚úÖ Inserted {len(documents)} documents (half-precision)")

    def insert_documents_full(self, documents: List[Tuple[str, str, np.ndarray]]):
        """
        Insert documents with full-precision embeddings.

        Args:
            documents: List of (title, content, embedding) tuples
        """
        with self.conn.cursor() as cur:
            execute_values(
                cur,
                """
                INSERT INTO documents_full (title, content, embedding)
                VALUES %s
                """,
                [(title, content, embedding.tolist()) for title, content, embedding in documents],
                template="(%s, %s, %s::vector)"  # Cast to vector
            )

        self.conn.commit()
        print(f"‚úÖ Inserted {len(documents)} documents (full-precision)")

    def query_half_precision(self, query_embedding: np.ndarray, limit: int = 10) -> List[Tuple]:
        """
        Query half-precision vectors for similar documents.

        Args:
            query_embedding: Query vector (1536 dimensions)
            limit: Number of results to return

        Returns:
            List of (id, title, distance) tuples
        """
        with self.conn.cursor() as cur:
            start_time = time.time()

            cur.execute("""
                SELECT id, title, embedding <=> %s::halfvec AS distance
                FROM documents_half
                ORDER BY embedding <=> %s::halfvec
                LIMIT %s
            """, (query_embedding.tolist(), query_embedding.tolist(), limit))

            results = cur.fetchall()
            query_time = (time.time() - start_time) * 1000  # ms

            print(f"Half-precision query time: {query_time:.2f}ms")
            return results

    def query_full_precision(self, query_embedding: np.ndarray, limit: int = 10) -> List[Tuple]:
        """
        Query full-precision vectors for similar documents.

        Args:
            query_embedding: Query vector (1536 dimensions)
            limit: Number of results to return

        Returns:
            List of (id, title, distance) tuples
        """
        with self.conn.cursor() as cur:
            start_time = time.time()

            cur.execute("""
                SELECT id, title, embedding <=> %s::vector AS distance
                FROM documents_full
                ORDER BY embedding <=> %s::vector
                LIMIT %s
            """, (query_embedding.tolist(), query_embedding.tolist(), limit))

            results = cur.fetchall()
            query_time = (time.time() - start_time) * 1000  # ms

            print(f"Full-precision query time: {query_time:.2f}ms")
            return results

    def migrate_full_to_half(self, batch_size: int = 1000):
        """
        Migrate existing full-precision embeddings to half-precision.

        This is a one-way migration. Backup data before running.

        Args:
            batch_size: Number of documents to migrate per batch
        """
        print("Starting migration from full to half precision...")

        with self.conn.cursor() as cur:
            # Get total count
            cur.execute("SELECT COUNT(*) FROM documents_full")
            total = cur.fetchone()[0]

            print(f"Migrating {total} documents...")

            # Migrate in batches
            offset = 0
            while offset < total:
                print(f"Migrating batch {offset}-{offset+batch_size}...")

                cur.execute("""
                    INSERT INTO documents_half (id, title, content, embedding, created_at)
                    SELECT id, title, content, embedding::halfvec, created_at
                    FROM documents_full
                    LIMIT %s OFFSET %s
                """, (batch_size, offset))

                self.conn.commit()
                offset += batch_size

        print("‚úÖ Migration complete")

    def compare_memory_usage(self):
        """
        Compare memory usage between full and half precision.

        Returns:
            Tuple of (full_size_mb, half_size_mb, savings_percent)
        """
        with self.conn.cursor() as cur:
            # Full precision table size
            cur.execute("""
                SELECT pg_size_pretty(pg_total_relation_size('documents_full')) AS size,
                       pg_total_relation_size('documents_full') AS bytes
            """)
            full_result = cur.fetchone()
            full_size_mb = full_result[1] / (1024 * 1024)

            # Half precision table size
            cur.execute("""
                SELECT pg_size_pretty(pg_total_relation_size('documents_half')) AS size,
                       pg_total_relation_size('documents_half') AS bytes
            """)
            half_result = cur.fetchone()
            half_size_mb = half_result[1] / (1024 * 1024)

            savings_percent = ((full_size_mb - half_size_mb) / full_size_mb) * 100

            print(f"\nüìä Memory Usage Comparison:")
            print(f"   Full precision:  {full_result[0]} ({full_size_mb:.2f} MB)")
            print(f"   Half precision:  {half_result[0]} ({half_size_mb:.2f} MB)")
            print(f"   Savings:         {savings_percent:.1f}%")

            return full_size_mb, half_size_mb, savings_percent

    def benchmark_accuracy(self, test_queries: List[np.ndarray], k: int = 10):
        """
        Benchmark accuracy difference between full and half precision.

        Uses NDCG (Normalized Discounted Cumulative Gain) to measure ranking quality.

        Args:
            test_queries: List of test query embeddings
            k: Number of results to compare (default 10)

        Returns:
            Average NDCG score (1.0 = perfect match, <1.0 = some ranking differences)
        """
        from sklearn.metrics import ndcg_score

        ndcg_scores = []

        for i, query in enumerate(test_queries):
            # Get results from both tables
            full_results = self.query_full_precision(query, limit=k)
            half_results = self.query_half_precision(query, limit=k)

            # Extract IDs and distances
            full_ids = [r[0] for r in full_results]
            half_ids = [r[0] for r in half_results]

            # Calculate overlap (what % of results are the same)
            overlap = len(set(full_ids) & set(half_ids)) / k

            ndcg_scores.append(overlap)

            if i == 0:  # Print first example
                print(f"\nExample query results:")
                print(f"Full precision top 3: {full_ids[:3]}")
                print(f"Half precision top 3: {half_ids[:3]}")
                print(f"Overlap: {overlap*100:.1f}%")

        avg_ndcg = np.mean(ndcg_scores)

        print(f"\nüìä Accuracy Benchmark (NDCG@{k}):")
        print(f"   Average score: {avg_ndcg:.4f}")
        print(f"   Accuracy loss: {(1 - avg_ndcg) * 100:.2f}%")

        if avg_ndcg > 0.99:
            print(f"   ‚úÖ Excellent accuracy (< 1% loss)")
        elif avg_ndcg > 0.95:
            print(f"   ‚úÖ Good accuracy (< 5% loss)")
        else:
            print(f"   ‚ö†Ô∏è  Significant accuracy loss (> 5%)")

        return avg_ndcg

    def close(self):
        """Close database connection."""
        self.conn.close()


# Example usage and benchmarking
def example_usage():
    """
    Complete example: create tables, insert data, query, and benchmark.
    """
    # Connection string (update with your credentials)
    CONN_STRING = "postgresql://apex:apexmemory2024@localhost:5432/apex_memory"

    # Initialize
    pgv = PgVectorHalfPrecision(CONN_STRING)

    try:
        # 1. Create tables
        print("=" * 60)
        print("Step 1: Creating tables")
        print("=" * 60)
        pgv.create_full_precision_table()
        pgv.create_half_precision_table()

        # 2. Generate sample embeddings (replace with real embeddings in production)
        print("\n" + "=" * 60)
        print("Step 2: Generating sample data")
        print("=" * 60)
        num_docs = 1000
        dim = 1536

        documents = [
            (
                f"Document {i}",
                f"This is the content of document {i}",
                np.random.randn(dim).astype(np.float32)  # Random embedding (replace with real)
            )
            for i in range(num_docs)
        ]

        # 3. Insert into both tables
        print("\n" + "=" * 60)
        print("Step 3: Inserting documents")
        print("=" * 60)
        pgv.insert_documents_full(documents)
        pgv.insert_documents_half(documents)

        # 4. Compare memory usage
        print("\n" + "=" * 60)
        print("Step 4: Memory comparison")
        print("=" * 60)
        pgv.compare_memory_usage()

        # 5. Query performance test
        print("\n" + "=" * 60)
        print("Step 5: Query performance")
        print("=" * 60)
        query_embedding = np.random.randn(dim).astype(np.float32)

        full_results = pgv.query_full_precision(query_embedding, limit=10)
        half_results = pgv.query_half_precision(query_embedding, limit=10)

        # 6. Accuracy benchmark
        print("\n" + "=" * 60)
        print("Step 6: Accuracy benchmark")
        print("=" * 60)
        test_queries = [np.random.randn(dim).astype(np.float32) for _ in range(10)]
        accuracy = pgv.benchmark_accuracy(test_queries, k=10)

        # 7. Summary
        print("\n" + "=" * 60)
        print("SUMMARY")
        print("=" * 60)
        print(f"Documents inserted: {num_docs}")
        print(f"Embedding dimensions: {dim}")
        print(f"Accuracy (NDCG): {accuracy:.4f}")
        print(f"Recommendation: {'‚úÖ Use half-precision' if accuracy > 0.99 else '‚ö†Ô∏è Test with your data'}")

    finally:
        pgv.close()


if __name__ == "__main__":
    example_usage()
