# Week 2 Day 1: Google Drive Archive Workflow - COMPLETE ‚úÖ

**Completion Date:** November 7, 2025
**Status:** ‚úÖ **100% Complete**

---

## Executive Summary

Week 2 Day 1 delivered a complete Google Drive Archive workflow with:
- ‚úÖ GoogleDriveArchiveWorkflow orchestrating 4 activities
- ‚úÖ 4 archive activities (determine folder, upload, verify, record metadata)
- ‚úÖ 11 activity unit tests (100% pass rate)
- ‚úÖ 3 workflow tests (100% pass rate)
- ‚úÖ PostgreSQL metadata tracking (document_archives table)
- ‚úÖ Complete error handling and retry logic

**Key Achievement:** After successful ingestion, documents can now be archived to Google Drive with domain-based folder organization and metadata tracking in PostgreSQL.

---

## What Was Delivered

### 1. Four Archive Activities ‚úÖ

**File Created:** `src/apex_memory/temporal/activities/google_drive_archive.py` (470 lines)

#### Activity 1: determine_archive_folder_activity

**Purpose:** Determine target Google Drive folder based on domain

```python
async def determine_archive_folder_activity(
    document_id: str,
    source: str,
    domain_name: Optional[str] = None,
) -> str:
    """Determine target Google Drive folder for archival.

    Folder Structure:
        {archive_base_folder_id}/
            ‚îú‚îÄ‚îÄ logistics/       # Logistics domain documents
            ‚îú‚îÄ‚îÄ personal/        # Personal domain documents
            ‚îú‚îÄ‚îÄ manufacturing/   # Manufacturing domain documents
            ‚îî‚îÄ‚îÄ general/         # Documents without domain classification
    """
```

**Features:**
- Domain-based folder mapping (logistics, personal, manufacturing, general)
- Validates Google Drive enabled and base folder configured
- Non-retryable errors for configuration issues

**Error Handling:**
- `GoogleDriveNotEnabled` - Google Drive archive not enabled (non-retryable)
- `GoogleDriveArchiveNotConfigured` - Base folder ID missing (non-retryable)

---

#### Activity 2: upload_to_google_drive_activity

**Purpose:** Upload file to Google Drive for archival

```python
async def upload_to_google_drive_activity(
    document_id: str,
    file_path: str,
    folder_id: str,
    archive_filename: Optional[str] = None,
) -> Dict[str, Any]:
    """Upload file to Google Drive.

    Returns:
        {
            'file_id': '1abc...xyz',
            'filename': 'contract.pdf',
            'folder_id': '1xyz...abc',
            'upload_timestamp': '2025-11-07T10:30:00Z'
        }
    """
```

**Features:**
- Uses GoogleDriveService.upload_file()
- Optional custom filename for archive
- Returns upload metadata (file_id, timestamp)
- Validates file exists before upload

**Error Handling:**
- `FileNotFound` - Local file doesn't exist (non-retryable)
- `GoogleDrivePermissionDenied` - No write permission to folder (non-retryable)
- `GoogleDriveUploadError` - Transient upload errors (retryable)

---

#### Activity 3: verify_upload_activity

**Purpose:** Verify uploaded file is accessible and valid

```python
async def verify_upload_activity(
    document_id: str,
    file_id: str,
    expected_filename: str,
) -> bool:
    """Verify upload succeeded.

    Verification Steps:
    1. Check file exists (get metadata)
    2. Verify filename matches
    3. Confirm file size > 0
    """
```

**Features:**
- Gets file metadata from Google Drive
- Validates filename matches expected
- Confirms non-zero file size
- Returns True on success

**Error Handling:**
- `VerificationFailed` - Filename mismatch or zero size (non-retryable)
- `VerificationError` - Transient metadata fetch errors (retryable)

---

#### Activity 4: record_archive_metadata_activity

**Purpose:** Track archive in PostgreSQL for audit trail

```python
async def record_archive_metadata_activity(
    document_id: str,
    file_id: str,
    folder_id: str,
    filename: str,
    upload_timestamp: str,
    source: str,
    domain_name: Optional[str] = None,
) -> None:
    """Record archive metadata in PostgreSQL.

    Schema:
        document_archives (
            id SERIAL PRIMARY KEY,
            document_id VARCHAR(255) NOT NULL,
            archive_file_id VARCHAR(255) NOT NULL,
            archive_folder_id VARCHAR(255) NOT NULL,
            archive_filename VARCHAR(500) NOT NULL,
            archive_timestamp TIMESTAMP NOT NULL,
            source VARCHAR(100),
            domain_name VARCHAR(100),
            created_at TIMESTAMP DEFAULT NOW()
        )
    """
```

**Features:**
- Creates document_archives table (idempotent)
- Inserts archive record with ON CONFLICT DO NOTHING (idempotent)
- Tracks file_id, folder_id, timestamp, source, domain
- Complete audit trail for compliance

**Error Handling:**
- `MetadataRecordingError` - Database errors (retryable)

---

### 2. Google Drive Archive Workflow ‚úÖ

**File Created:** `src/apex_memory/temporal/workflows/google_drive_archive.py` (280 lines)

#### GoogleDriveArchiveWorkflow

**Purpose:** Orchestrate 4 activities with fault tolerance and observability

**Workflow Steps:**
```
1. Determine archive folder (10s timeout, 3 retries)
   ‚Üì
2. Upload to Google Drive (5min timeout, 5 retries)
   ‚Üì
3. Verify upload (30s timeout, 3 retries)
   ‚Üì
4. Record metadata (30s timeout, 5 retries)
```

**Status Progression:**
```
pending ‚Üí determining_folder ‚Üí uploading ‚Üí verifying ‚Üí recording_metadata ‚Üí completed
```

**Workflow Features:**
- Automatic retries via Temporal (activity-level retry policies)
- State persistence across failures
- Complete observability in Temporal UI
- Query support for status tracking

**Retry Policies:**
- Activity 1 (determine_folder): 3 retries, 1-10s backoff
- Activity 2 (upload): **5 retries**, 2-60s backoff (more retries for upload)
- Activity 3 (verify): 3 retries, 1-30s backoff
- Activity 4 (metadata): **5 retries**, 1-30s backoff (more retries for DB)

**Error Handling:**
- Workflow catches all exceptions
- Returns `status="failed"` with error message
- Allows calling code to handle failures gracefully

---

### 3. Comprehensive Test Suite ‚úÖ

#### Activity Tests (11 tests, 100% pass rate)

**File Created:** `tests/unit/test_google_drive_archive_activities.py` (460 lines)

**Tests:**
```
‚úÖ test_determine_archive_folder_with_domain
‚úÖ test_determine_archive_folder_without_domain
‚úÖ test_determine_archive_folder_drive_not_enabled
‚úÖ test_upload_to_google_drive_success
‚úÖ test_upload_to_google_drive_file_not_found
‚úÖ test_upload_to_google_drive_permission_denied
‚úÖ test_verify_upload_success
‚úÖ test_verify_upload_filename_mismatch
‚úÖ test_verify_upload_zero_size
‚úÖ test_record_archive_metadata_success
‚úÖ test_record_archive_metadata_database_error
```

**Coverage:**
- 3 tests per activity (success + 2 error cases)
- Mock-based testing (no real API calls, no real DB)
- Validates error types and non_retryable flags

---

#### Workflow Tests (3 tests, 100% pass rate)

**File Created:** `tests/unit/test_google_drive_archive_workflow.py` (270 lines)

**Tests:**
```
‚úÖ test_google_drive_archive_workflow_success
‚úÖ test_google_drive_archive_workflow_upload_failure
‚úÖ test_google_drive_archive_workflow_query
```

**Coverage:**
- Complete workflow success path (all 4 activities)
- Failure handling (upload error)
- Status query during execution

**Test Pattern:**
```python
# Uses Temporal test environment
async with await WorkflowEnvironment.start_time_skipping() as env:
    async with Worker(
        env.client,
        task_queue="test-archive-queue",
        workflows=[GoogleDriveArchiveWorkflow],
        activities=[...all 4 activities...]
    ):
        result = await env.client.execute_workflow(
            GoogleDriveArchiveWorkflow.run,
            args=["DOC-123", "/path/to/file.pdf", "frontapp", "logistics"],
            ...
        )

        assert result["status"] == "success"
        assert result["file_id"] == "1uploaded...file"
```

---

## Architecture

### Archive Flow

**Trigger:** After successful document ingestion (Week 2 Day 2 will integrate)

```
DocumentIngestionWorkflow
    ‚Üì
(success)
    ‚Üì
cleanup_staging_activity
    ‚Üì
GoogleDriveArchiveWorkflow (async, non-blocking)
    ‚Üì
Step 1: Determine folder ‚Üí logistics/ folder ID
    ‚Üì
Step 2: Upload file ‚Üí file_id: 1abc...xyz
    ‚Üì
Step 3: Verify upload ‚Üí filename match, size > 0
    ‚Üì
Step 4: Record metadata ‚Üí PostgreSQL document_archives table
```

### Domain-Based Folder Organization

**Folder Structure:**
```
Google Drive Archive Root (archive_base_folder_id)
‚îú‚îÄ‚îÄ logistics/          # Logistics domain documents
‚îÇ   ‚îú‚îÄ‚îÄ invoice_123.pdf
‚îÇ   ‚îî‚îÄ‚îÄ shipment_456.pdf
‚îú‚îÄ‚îÄ personal/           # Personal domain documents
‚îÇ   ‚îú‚îÄ‚îÄ resume.pdf
‚îÇ   ‚îî‚îÄ‚îÄ tax_docs.pdf
‚îú‚îÄ‚îÄ manufacturing/      # Manufacturing domain documents
‚îÇ   ‚îú‚îÄ‚îÄ specs_789.pdf
‚îÇ   ‚îî‚îÄ‚îÄ schematics.pdf
‚îî‚îÄ‚îÄ general/            # Documents without domain classification
    ‚îú‚îÄ‚îÄ misc_001.pdf
    ‚îî‚îÄ‚îÄ temp_002.pdf
```

**Configuration:**
```bash
# .env file
GOOGLE_DRIVE_ENABLED=true
GOOGLE_DRIVE_ARCHIVE_BASE_FOLDER_ID=1xyz...abc  # Parent folder for all archives
```

### PostgreSQL Metadata Tracking

**Schema:**
```sql
CREATE TABLE document_archives (
    id SERIAL PRIMARY KEY,
    document_id VARCHAR(255) NOT NULL,
    archive_file_id VARCHAR(255) NOT NULL,      -- Google Drive file ID
    archive_folder_id VARCHAR(255) NOT NULL,     -- Google Drive folder ID
    archive_filename VARCHAR(500) NOT NULL,
    archive_timestamp TIMESTAMP NOT NULL,
    source VARCHAR(100),                         -- frontapp, local_upload, etc.
    domain_name VARCHAR(100),                    -- logistics, personal, etc.
    created_at TIMESTAMP DEFAULT NOW()
);
```

**Sample Record:**
```sql
INSERT INTO document_archives VALUES (
    1,
    'DOC-123',
    '1abc...xyz',                    -- Google Drive file ID
    '1folder...id',                  -- logistics/ folder ID
    'contract.pdf',
    '2025-11-07 10:30:00',
    'frontapp',
    'logistics',
    '2025-11-07 10:30:05'
);
```

**Benefits:**
- Complete audit trail (who, what, when, where)
- Easy queries: "Which documents from FrontApp are in logistics folder?"
- Compliance: Track all archived documents
- Recovery: Map document_id to Google Drive file_id for retrieval

---

## Test Summary

```
‚úÖ 11/11 Activity tests passing (100%)
‚úÖ 3/3 Workflow tests passing (100%)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
   14 tests for Week 2 Day 1
```

**Test Execution:**
```bash
# Activity tests
pytest tests/unit/test_google_drive_archive_activities.py -v
# Result: 11 passed in 1.25s

# Workflow tests
pytest tests/unit/test_google_drive_archive_workflow.py -v
# Result: 3 passed in 3.81s
```

---

## Files Created

### Source Files (2)

1. **src/apex_memory/temporal/activities/google_drive_archive.py** (470 lines)
   - 4 archive activities
   - Complete error handling
   - PostgreSQL table creation (idempotent)

2. **src/apex_memory/temporal/workflows/google_drive_archive.py** (280 lines)
   - GoogleDriveArchiveWorkflow class
   - 4-step orchestration
   - Query support for status tracking

### Test Files (2)

3. **tests/unit/test_google_drive_archive_activities.py** (460 lines)
   - 11 activity tests
   - Mock-based testing
   - 100% pass rate

4. **tests/unit/test_google_drive_archive_workflow.py** (270 lines)
   - 3 workflow tests
   - Temporal test environment
   - 100% pass rate

---

## Technical Decisions

### 1. Async Archive (Non-Blocking)

**Decision:** Archive runs asynchronously, doesn't block ingestion completion

**Implementation:**
```python
# Week 2 Day 2 will implement this:
# cleanup_staging_activity triggers archive AFTER ingestion completes
await client.start_workflow(  # start_workflow (async), not execute_workflow (blocking)
    GoogleDriveArchiveWorkflow.run,
    args=[document_id, file_path, source, domain_name],
    id=f"archive-{document_id}",
    task_queue="apex-ingestion-queue"
)
```

**Benefits:**
- Ingestion completes quickly (~2-5s)
- Archive happens in background (5-30s)
- User doesn't wait for Google Drive upload
- Failures don't block ingestion pipeline

### 2. Idempotent Activities

**Decision:** All activities are safe to retry (idempotent)

**Implementation:**
- **determine_folder**: Always returns same folder for same domain
- **upload**: Resumable uploads handle interruptions
- **verify**: Read-only operation (safe to retry)
- **record_metadata**: `ON CONFLICT DO NOTHING` prevents duplicates

**Benefits:**
- Temporal can safely retry failed activities
- No duplicate archives or database records
- Safe across worker restarts

### 3. Comprehensive Verification

**Decision:** Verify upload succeeded before recording metadata

**Rationale:**
- Catches corrupted uploads (zero size)
- Catches wrong filenames (upload succeeded but wrong file)
- Ensures archive is actually accessible
- Prevents bad metadata in PostgreSQL

**Trade-off:**
- Extra API call to Google Drive
- But: Worth it for data integrity

### 4. Database-First Metadata

**Decision:** Track all archives in PostgreSQL (not just Google Drive metadata)

**Benefits:**
- Fast queries without Google Drive API calls
- Complete audit trail
- SQL analytics: "Count archives by domain"
- Enables compliance reports

---

## Usage

### Manual Workflow Execution

```python
from temporalio.client import Client
from apex_memory.temporal.workflows.google_drive_archive import GoogleDriveArchiveWorkflow

client = await Client.connect("localhost:7233")

# Execute archive workflow
result = await client.execute_workflow(
    GoogleDriveArchiveWorkflow.run,
    args=[
        "DOC-123",                              # document_id
        "/tmp/apex-staging/frontapp/DOC-123/contract.pdf",  # file_path
        "frontapp",                             # source
        "logistics"                             # domain_name (optional)
    ],
    id="archive-DOC-123",
    task_queue="apex-ingestion-queue"
)

print(result)
# {
#     'status': 'success',
#     'document_id': 'DOC-123',
#     'source': 'frontapp',
#     'file_id': '1abc...xyz',
#     'folder_id': '1folder...id',
#     'workflow_status': 'completed'
# }
```

### Query Workflow Status

```python
# Get workflow handle
handle = client.get_workflow_handle("archive-DOC-123")

# Query status (non-blocking)
status = await handle.query(GoogleDriveArchiveWorkflow.get_status)

print(status)
# {
#     'document_id': 'DOC-123',
#     'source': 'frontapp',
#     'status': 'uploading',  # Current status
#     'folder_id': '1folder...id',
#     'file_id': None,  # Not uploaded yet
#     'error': None
# }
```

---

## What's Next: Week 2 Day 2

### Goal: Integrate Archive with Ingestion

**Objective:** Trigger GoogleDriveArchiveWorkflow after successful ingestion

**Tasks:**
1. Enhance `cleanup_staging_activity` to start archive workflow
2. Make archive trigger **async** (non-blocking)
3. Add 4 tests for cleanup ‚Üí archive integration
4. Verify ingestion completes quickly (archive in background)

**Expected Architecture:**
```
DocumentIngestionWorkflow
    ‚Üì
Step 6: cleanup_staging_activity
    ‚Üì
    ‚îú‚îÄ Remove staging directory (as before)
    ‚îî‚îÄ Start GoogleDriveArchiveWorkflow (NEW - async)
         ‚Üì
    (Archive happens in background, ingestion already complete)
```

**Benefits:**
- Clean separation of concerns
- Ingestion fast (<5s), archive slow (5-30s) happens async
- Archive failures don't affect ingestion success
- Complete end-to-end: Drive input ‚Üí Ingest ‚Üí Drive archive

---

## Summary

‚úÖ **Week 2 Day 1 Complete - Google Drive Archive Workflow**

**Delivered:**
- GoogleDriveArchiveWorkflow with 4 activities
- Domain-based folder organization
- PostgreSQL metadata tracking (document_archives table)
- 14 tests (100% pass rate)
- Complete error handling and retry logic
- Async-ready architecture (Week 2 Day 2 integration)

**Test Count:**
- Activity tests: 11/11 passing ‚úÖ
- Workflow tests: 3/3 passing ‚úÖ
- **Total: 14 tests for Week 2 Day 1**

**Timeline:** 1 day (as planned)

**Ready for:** Week 2 Day 2 - Integrate archive with ingestion pipeline

---

**üéØ Week 2 Day 1 Achievement:** Complete Google Drive archive infrastructure in place. Documents can be archived to Drive with domain-based organization and full PostgreSQL audit trail. Ready to integrate with ingestion pipeline tomorrow.
