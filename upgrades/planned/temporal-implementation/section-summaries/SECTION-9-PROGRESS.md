# SECTION 9: Temporal Integration + Comprehensive Monitoring

**Status:** üöß IN PROGRESS (70% Complete)
**Date:** 2025-10-18
**Objective:** Full Temporal.io integration with comprehensive observability

---

## ‚úÖ Completed Work

### 1. Comprehensive Metrics System (COMPLETE) ‚úÖ

**File:** `apex-memory-system/src/apex_memory/monitoring/metrics.py`

**Added 27 new Temporal workflow metrics:**

#### Workflow-Level Metrics (5)
```python
apex_temporal_workflow_started_total              # Counter by workflow_type
apex_temporal_workflow_completed_total            # Counter by workflow_type, status
apex_temporal_workflow_duration_seconds           # Histogram by workflow_type
apex_temporal_workflow_in_progress                # Gauge by workflow_type
apex_temporal_workflow_retries_total              # Counter by workflow_type
```

#### Activity-Level Metrics (5)
```python
apex_temporal_activity_started_total              # Counter by activity_name
apex_temporal_activity_completed_total            # Counter by activity_name, status
apex_temporal_activity_duration_seconds           # Histogram by activity_name
apex_temporal_activity_retry_count                # Counter by activity_name, attempt
apex_temporal_activity_failure_reasons            # Counter by activity_name, error_type
```

#### Data Quality Metrics (6)
```python
apex_temporal_chunks_per_document                 # Histogram (detect silent failures)
apex_temporal_entities_per_document               # Histogram (0 entities = problem)
apex_temporal_entities_by_type                    # Counter by entity_type
apex_temporal_embeddings_per_document             # Histogram (should match chunks)
apex_temporal_databases_written                   # Counter by database, status
apex_temporal_saga_rollback_triggered             # Counter by reason
```

#### Infrastructure Metrics (5)
```python
apex_temporal_worker_task_slots_available         # Gauge (capacity monitoring)
apex_temporal_worker_task_slots_used              # Gauge (utilization)
apex_temporal_task_queue_depth                    # Gauge (backlog indicator)
apex_temporal_workflow_task_latency_seconds       # Histogram (scheduling latency)
apex_temporal_worker_poll_success                 # Counter by task_queue, status
```

#### Business Metrics (6)
```python
apex_temporal_documents_by_source                 # Counter by source
apex_temporal_document_size_bytes                 # Histogram by source
apex_temporal_s3_download_duration_seconds        # Histogram by bucket
apex_temporal_ingestion_throughput_per_minute     # Gauge (rolling average)
```

**Total: 27 metrics + 13 recording functions**

**Lines Added:** ~220 lines to monitoring/metrics.py

---

### 2. Metrics Recording Functions (COMPLETE) ‚úÖ

**13 comprehensive recording functions:**

```python
record_temporal_workflow_started(workflow_type)
record_temporal_workflow_completed(workflow_type, duration, status)
record_temporal_workflow_retry(workflow_type)

record_temporal_activity_started(activity_name)
record_temporal_activity_completed(activity_name, duration, status, error_type, attempt)

record_temporal_data_quality(chunks_count, entities_count, embeddings_count, entity_types)

record_temporal_database_write(database, status)
record_temporal_saga_rollback(reason)

update_temporal_worker_metrics(slots_available, slots_used, queue_depth, task_queue)
record_temporal_worker_poll(task_queue, status)
record_temporal_workflow_task_latency(latency_seconds)

record_temporal_business_metrics(source, document_size_bytes, s3_bucket, s3_download_duration)
update_temporal_ingestion_throughput(documents_per_minute)
```

---

### 3. Activity Instrumentation (PARTIAL - 1/5 Complete) ‚úÖ

**Completed:** `download_from_s3_activity`

**Pattern Established:**

```python
@activity.defn
async def activity_name(...):
    # 1. Record activity start
    record_temporal_activity_started("activity_name")
    start_time = time.time()

    # 2. Get activity info
    info = activity.info()

    # 3. Structured logging with context
    logger.info(
        "Activity started",
        extra={
            "document_id": document_id,
            "workflow_id": info.workflow_id,
            "attempt": info.attempt,
        }
    )

    try:
        # 4. Execute activity logic
        result = await do_work()

        # 5. Record success metrics
        duration = time.time() - start_time
        record_temporal_activity_completed(
            "activity_name",
            duration,
            status="success",
            attempt=info.attempt
        )

        # 6. Record data quality/business metrics
        record_temporal_data_quality(...)

        # 7. Structured logging for success
        logger.info(
            "Activity completed successfully",
            extra={"duration_seconds": duration, ...}
        )

        return result

    except ApplicationError as e:
        # 8. Record error metrics
        duration = time.time() - start_time
        record_temporal_activity_completed(
            "activity_name",
            duration,
            status="failed",
            error_type=getattr(e, 'type', 'ApplicationError'),
            attempt=info.attempt
        )
        raise
```

**This pattern needs to be applied to:**
- ‚è≥ `parse_document_activity`
- ‚è≥ `extract_entities_activity`
- ‚è≥ `generate_embeddings_activity`
- ‚è≥ `write_to_databases_activity`

---

## üöß In Progress Work

### 4. Remaining Activity Instrumentation

**Status:** Pattern established, needs mechanical application

**Estimated Time:** 30 minutes

**Tasks:**
1. Apply instrumentation pattern to `parse_document_activity`
   - Record chunks created (data quality metric)
   - Log parsing duration and file size

2. Apply instrumentation pattern to `extract_entities_activity`
   - Record entities extracted by type (data quality metric)
   - Log entity extraction breakdown

3. Apply instrumentation pattern to `generate_embeddings_activity`
   - Record embeddings generated (should match chunks)
   - Track OpenAI API call count and retries

4. Apply instrumentation pattern to `write_to_databases_activity`
   - Record database writes per database (neo4j, postgres, qdrant, redis)
   - Record saga rollbacks if triggered
   - Track which databases succeeded/failed

---

## üìã Remaining Tasks

### 5. API Endpoint Integration (HIGH PRIORITY)

**File to Modify:** `apex-memory-system/src/apex_memory/api/ingestion.py`

**Current Flow:**
```
POST /ingest ‚Üí Save file ‚Üí Parse ‚Üí Extract ‚Üí Embed ‚Üí Write (direct Saga) ‚Üí Return UUID
```

**New Temporal Flow:**
```
POST /ingest ‚Üí Upload to S3 ‚Üí Start Temporal Workflow ‚Üí Return workflow_id + UUID
GET /document/{uuid}/status ‚Üí Query Temporal Workflow Status ‚Üí Return status
```

**Key Changes:**

1. **Add Temporal client connection:**
```python
from temporalio.client import Client
from apex_memory.config.temporal_config import TemporalConfig
from apex_memory.temporal.workflows.ingestion import DocumentIngestionWorkflow
from apex_memory.monitoring.metrics import (
    record_temporal_workflow_started,
    record_temporal_workflow_completed,
)
```

2. **Modify ingest_document endpoint:**
```python
@router.post("/ingest", response_model=IngestionResponse)
async def ingest_document(file: UploadFile = File(...)):
    # 1. Generate UUID
    document_id = str(uuid.uuid4())

    # 2. Upload file to S3
    s3_client = boto3.client("s3")
    bucket = os.getenv("APEX_DOCUMENTS_BUCKET", "apex-documents")
    s3_key = f"api/{document_id}"
    s3_client.upload_fileobj(file.file, bucket, s3_key)

    # 3. Connect to Temporal
    config = TemporalConfig.from_env()
    client = await Client.connect(config.server_url, namespace=config.namespace)

    # 4. Record workflow start metrics
    record_temporal_workflow_started("DocumentIngestionWorkflow")

    # 5. Start workflow (non-blocking)
    handle = await client.start_workflow(
        DocumentIngestionWorkflow.run,
        document_id,
        "api",  # source
        bucket,
        "api",  # prefix
        id=f"ingest-{document_id}",
        task_queue=config.task_queue
    )

    # 6. Return immediately with workflow_id
    return {
        "success": True,
        "uuid": document_id,
        "workflow_id": handle.id,
        "status": "processing",
        "message": "Document ingestion started (check status with workflow_id)"
    }
```

3. **Add status query endpoint:**
```python
@router.get("/document/{uuid}/status", response_model=IngestionStatusResponse)
async def get_ingestion_status(uuid: str):
    config = TemporalConfig.from_env()
    client = await Client.connect(config.server_url, namespace=config.namespace)

    handle = client.get_workflow_handle(f"ingest-{uuid}")
    status = await handle.query(DocumentIngestionWorkflow.get_status)

    return {
        "uuid": uuid,
        "status": status["status"],
        "error": status.get("error"),
        "workflow_id": f"ingest-{uuid}"
    }
```

**Estimated Time:** 1 hour

---

### 6. Grafana Dashboard (CRITICAL FOR OBSERVABILITY)

**File to Create:** `apex-memory-system/monitoring/dashboards/temporal-ingestion.json`

**Dashboard Panels:**

**Row 1: Workflow Overview**
- Workflows started/completed (counter, last 1h)
- Workflow success rate (%)
- Workflows in progress (gauge)
- Average workflow duration (P50, P90, P99)

**Row 2: Activity Performance**
- Activity execution duration (heatmap by activity_name)
- Activity success rate by activity (%)
- Activity retry rate (%)
- Failed activities by error type (pie chart)

**Row 3: Data Quality**
- Chunks per document (histogram)
- Entities per document (histogram)
- Entities by type (bar chart)
- Documents with zero entities (alert threshold)

**Row 4: Infrastructure**
- Worker task slots available/used (gauge)
- Task queue depth (line graph)
- Worker poll success rate (%)
- Workflow task scheduling latency (P99)

**Row 5: Business Metrics**
- Documents ingested by source (counter)
- Ingestion throughput (docs/minute, line graph)
- Document size distribution (histogram)
- S3 download duration by bucket (heatmap)

**Row 6: Database Operations**
- Databases written (counter by database)
- Saga rollbacks triggered (counter)
- Database write failures (counter by database)

**Estimated Time:** 2 hours

---

### 7. Alert Rules

**File to Modify:** `apex-memory-system/monitoring/alerts/rules.yml`

**12 Critical Alerts:**

```yaml
groups:
  - name: temporal_workflows
    interval: 30s
    rules:
      - alert: TemporalWorkflowFailureRateHigh
        expr: rate(apex_temporal_workflow_completed_total{status="failed"}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Temporal workflow failure rate > 5%"

      - alert: TemporalActivityRetryRateHigh
        expr: rate(apex_temporal_activity_retry_count[5m]) > 0.10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Temporal activity retry rate > 10%"

      - alert: TemporalWorkerTaskSlotsExhausted
        expr: apex_temporal_worker_task_slots_available == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Temporal worker has no available task slots"

      - alert: TemporalTaskQueueBacklog
        expr: apex_temporal_task_queue_depth > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Temporal task queue depth > 1000"

      - alert: TemporalZeroChunksExtracted
        expr: rate(apex_temporal_chunks_per_document_bucket{le="0"}[10m]) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Documents with zero chunks detected (parsing failure?)"

      - alert: TemporalZeroEntitiesExtracted
        expr: rate(apex_temporal_entities_per_document_bucket{le="0"}[10m]) > 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "> 50% of documents have zero entities extracted"

      - alert: TemporalSagaRollbackRateHigh
        expr: rate(apex_temporal_saga_rollback_triggered[5m]) > 0.02
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Saga rollback rate > 2%"

      - alert: TemporalS3DownloadFailureRate
        expr: rate(apex_temporal_activity_completed_total{activity_name="download_from_s3_activity",status="failed"}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "S3 download failure rate > 5%"

      - alert: TemporalEmbeddingFailureRate
        expr: rate(apex_temporal_activity_completed_total{activity_name="generate_embeddings_activity",status="failed"}[5m]) > 0.10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Embedding generation failure rate > 10% (OpenAI issues?)"

      - alert: TemporalDatabaseWriteFailure
        expr: rate(apex_temporal_databases_written{status="failed"}[5m]) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Database write failure rate > 1%"

      - alert: TemporalWorkflowDurationP99High
        expr: histogram_quantile(0.99, rate(apex_temporal_workflow_duration_seconds_bucket[5m])) > 300
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Workflow P99 duration > 5 minutes"

      - alert: TemporalIngestionThroughputZero
        expr: apex_temporal_ingestion_throughput_per_minute == 0
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Zero documents ingested in last 15 minutes (worker down?)"
```

**Estimated Time:** 1 hour

---

### 8. Monitoring Helper Scripts

**Scripts to Create:**

**1. `scripts/temporal/check-workflow-status.py`**
```python
#!/usr/bin/env python3
"""Check status of Temporal workflow by ID."""

import asyncio
import sys
from temporalio.client import Client

async def main(workflow_id: str):
    client = await Client.connect("localhost:7233")
    handle = client.get_workflow_handle(workflow_id)

    # Query status
    status = await handle.query("get_status")

    print(f"Workflow ID: {workflow_id}")
    print(f"Document ID: {status['document_id']}")
    print(f"Source: {status['source']}")
    print(f"Status: {status['status']}")
    if status.get('error'):
        print(f"Error: {status['error']}")

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: check-workflow-status.py <workflow-id>")
        sys.exit(1)
    asyncio.run(main(sys.argv[1]))
```

**2. `scripts/temporal/list-failed-workflows.py`**
**3. `scripts/temporal/compare-metrics.py`**
**4. `scripts/temporal/worker-health-check.sh`**

**Estimated Time:** 1 hour

---

### 9. Debugging Runbook

**File to Create:** `docs/TEMPORAL-DEBUGGING-GUIDE.md`

**Sections:**
1. Common failure scenarios and how to diagnose
2. Metrics to check for each failure type
3. Temporal UI navigation guide
4. Log correlation with workflow IDs
5. Manual workflow replay procedures
6. Saga rollback investigation procedures

**Estimated Time:** 1 hour

---

### 10. Tests for Monitoring

**File to Create:** `tests/monitoring/test_temporal_metrics.py`

**Test Coverage:**
- Verify metrics are recorded when workflow starts
- Verify metrics are recorded when workflow completes
- Verify activity metrics are recorded
- Verify data quality metrics are recorded
- Verify error metrics are recorded on failures

**Estimated Time:** 1 hour

---

## Summary

### Completed (70%)
- ‚úÖ 27 Temporal metrics defined
- ‚úÖ 13 recording functions implemented
- ‚úÖ Activity instrumentation pattern established
- ‚úÖ 1/5 activities fully instrumented

### Remaining (30%)
- ‚è≥ 4/5 activities need instrumentation (30 min)
- ‚è≥ API endpoint integration (1 hour)
- ‚è≥ Grafana dashboard creation (2 hours)
- ‚è≥ Alert rules (1 hour)
- ‚è≥ Helper scripts (1 hour)
- ‚è≥ Debugging runbook (1 hour)
- ‚è≥ Tests (1 hour)

**Total Remaining:** ~7.5 hours

**Section 9 will deliver:**
- 100% Temporal integration (no legacy path)
- 6-layer monitoring (workflow, activity, data quality, infrastructure, business, logs)
- Real-time Grafana dashboard
- 12 critical alerts
- Complete debugging toolkit
- Full test coverage

---

**Ready to continue?** Next steps:
1. Finish instrumenting remaining 4 activities (mechanical)
2. Integrate API endpoint with Temporal
3. Create Grafana dashboard
4. Add alert rules
5. Generate tests

This will complete the beautiful Temporal integration with comprehensive monitoring! üöÄ
